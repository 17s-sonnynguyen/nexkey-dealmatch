{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c90a0a-c4b6-490a-acb8-751d27790998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "np.random.seed(7)\n",
    "torch.manual_seed(7)\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e504991d-52ec-4dea-9992-07d8a5ea93ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries: (30000, 16)\n",
      "properties: (15000, 27)\n",
      "interactions: (480000, 4)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../data/nexkey_synthetic_dataset_v1\"\n",
    "\n",
    "queries = pd.read_csv(f\"{DATA_PATH}/queries.csv\")\n",
    "properties = pd.read_csv(f\"{DATA_PATH}/properties.csv\")\n",
    "interactions = pd.read_csv(f\"{DATA_PATH}/interactions.csv\")\n",
    "\n",
    "print(\"queries:\", queries.shape)\n",
    "print(\"properties:\", properties.shape)\n",
    "print(\"interactions:\", interactions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "540ca4f7-c3a2-4a6e-ad01-8d585fdfa037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_int: (384000, 4)\n",
      "val_int  : (48000, 4)\n",
      "test_int : (48000, 4)\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(7)\n",
    "\n",
    "all_qids = queries[\"query_id\"].unique()\n",
    "rng.shuffle(all_qids)\n",
    "\n",
    "n = len(all_qids)\n",
    "train_qids = set(all_qids[:int(0.80*n)])\n",
    "val_qids   = set(all_qids[int(0.80*n):int(0.90*n)])\n",
    "test_qids  = set(all_qids[int(0.90*n):])\n",
    "\n",
    "train_int = interactions[interactions[\"query_id\"].isin(train_qids)].copy()\n",
    "val_int   = interactions[interactions[\"query_id\"].isin(val_qids)].copy()\n",
    "test_int  = interactions[interactions[\"query_id\"].isin(test_qids)].copy()\n",
    "\n",
    "print(\"train_int:\", train_int.shape)\n",
    "print(\"val_int  :\", val_int.shape)\n",
    "print(\"test_int :\", test_int.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22f17c40-ff8b-4567-857f-69b8703d0b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>property_id</th>\n",
       "      <th>deal_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Subto Single Family in Raleigh NC. 4 bed 1.0 b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Hybrid Single Family in Sacramento CA. 5 bed 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Cash Condo in Charleston SC. 4 bed 2.5 bath, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Subto Manufactured in Greenville AL. 4 bed 2.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Seller Finance Single Family in Fairview AL. 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   property_id                                          deal_text\n",
       "0            1  Subto Single Family in Raleigh NC. 4 bed 1.0 b...\n",
       "1            2  Hybrid Single Family in Sacramento CA. 5 bed 1...\n",
       "2            3  Cash Condo in Charleston SC. 4 bed 2.5 bath, 3...\n",
       "3            4  Subto Manufactured in Greenville AL. 4 bed 2.0...\n",
       "4            5  Seller Finance Single Family in Fairview AL. 3..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def property_to_text(row):\n",
    "    return (\n",
    "        f\"{row['deal_type']} {row['property_type']} in {row['city']} {row['state']}. \"\n",
    "        f\"{int(row['beds'])} bed {row['baths']} bath, {int(row['sqft'])} sqft. \"\n",
    "        f\"Purchase {int(row['purchase_price'])}, ARV {int(row['arv'])}, \"\n",
    "        f\"Entry {int(row['entry_fee'])}, Payment {row['estimated_monthly_payment']}. \"\n",
    "        f\"Condition {row['condition']}, Occupancy {row['occupancy']}.\"\n",
    "    )\n",
    "\n",
    "properties[\"deal_text\"] = properties.apply(property_to_text, axis=1)\n",
    "properties[[\"property_id\", \"deal_text\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b8e790c-c992-45f9-8974-4d9b98bc7f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dual_vocab size : 18145\n",
      "cross_vocab size: 18146\n",
      "PAD/UNK/SEP: 0 1 18145\n"
     ]
    }
   ],
   "source": [
    "CKPT_PATH = \"../models/checkpoints\"\n",
    "\n",
    "with open(f\"{CKPT_PATH}/dual_vocab_v1.json\", \"r\") as f:\n",
    "    dual_vocab = json.load(f)\n",
    "\n",
    "cross_vocab = copy.deepcopy(dual_vocab)\n",
    "if \"<SEP>\" not in cross_vocab:\n",
    "    cross_vocab[\"<SEP>\"] = len(cross_vocab)\n",
    "\n",
    "PAD_ID = cross_vocab[\"<PAD>\"]\n",
    "UNK_ID = cross_vocab[\"<UNK>\"]\n",
    "SEP_ID = cross_vocab[\"<SEP>\"]\n",
    "\n",
    "print(\"dual_vocab size :\", len(dual_vocab))\n",
    "print(\"cross_vocab size:\", len(cross_vocab))\n",
    "print(\"PAD/UNK/SEP:\", PAD_ID, UNK_ID, SEP_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1494aeeb-7a86-43ff-ab57-7c16cda145be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_text_map: 30000\n",
      "deal_text_map : 15000\n"
     ]
    }
   ],
   "source": [
    "query_text_map = queries.set_index(\"query_id\")[\"query_text\"].to_dict()\n",
    "deal_text_map  = properties.set_index(\"property_id\")[\"deal_text\"].to_dict()\n",
    "\n",
    "print(\"query_text_map:\", len(query_text_map))\n",
    "print(\"deal_text_map :\", len(deal_text_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa4fdb5f-89ef-44f3-b061-52708e8a598c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before sampling:\n",
      "Train size: 384000\n",
      "Val size  : 48000\n",
      "\n",
      "Train relevance counts:\n",
      " relevance\n",
      "0    192000\n",
      "2     72000\n",
      "1     72000\n",
      "3     48000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After sampling:\n",
      "Train size: 250000\n",
      "Val size  : 48000\n"
     ]
    }
   ],
   "source": [
    "MAX_TRAIN = 250_000\n",
    "MAX_VAL   = 50_000\n",
    "\n",
    "train_df = train_int.copy()\n",
    "val_df   = val_int.copy()\n",
    "\n",
    "print(\"Before sampling:\")\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Val size  :\", len(val_df))\n",
    "print(\"\\nTrain relevance counts:\\n\", train_df[\"relevance\"].value_counts())\n",
    "\n",
    "if len(train_df) > MAX_TRAIN:\n",
    "    train_df = train_df.sample(MAX_TRAIN, random_state=7)\n",
    "\n",
    "if len(val_df) > MAX_VAL:\n",
    "    val_df = val_df.sample(MAX_VAL, random_state=7)\n",
    "\n",
    "print(\"\\nAfter sampling:\")\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Val size  :\", len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6a68d7f-06ec-4e8b-9242-c50e71596c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str):\n",
    "    return re.findall(r\"[a-z0-9]+\", str(text).lower())\n",
    "\n",
    "def encode_pair_cross(query_text: str, deal_text: str, max_len: int = 96):\n",
    "    q_ids = [cross_vocab.get(w, UNK_ID) for w in tokenize(query_text)]\n",
    "    d_ids = [cross_vocab.get(w, UNK_ID) for w in tokenize(deal_text)]\n",
    "\n",
    "    q_max = int(max_len * 0.45)\n",
    "    d_max = max_len - q_max - 1  # reserve 1 for <SEP>\n",
    "\n",
    "    q_ids = q_ids[:q_max]\n",
    "    d_ids = d_ids[:d_max]\n",
    "\n",
    "    ids = q_ids + [SEP_ID] + d_ids\n",
    "    if len(ids) < max_len:\n",
    "        ids += [PAD_ID] * (max_len - len(ids))\n",
    "\n",
    "    ids = np.array(ids, dtype=np.int64)\n",
    "\n",
    "    # Safety: keep IDs always valid for embedding lookup\n",
    "    ids = np.clip(ids, 0, len(cross_vocab) - 1)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58d82d6b-3b7d-403a-aa88-61a4e1ae661c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class ready âœ…\n"
     ]
    }
   ],
   "source": [
    "class CrossEncoderDataset(Dataset):\n",
    "    def __init__(self, df, query_text_map, deal_text_map, max_len=96):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.qmap = query_text_map\n",
    "        self.dmap = deal_text_map\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        qid = int(row[\"query_id\"])\n",
    "        pid = int(row[\"property_id\"])\n",
    "        y   = int(row[\"relevance\"])\n",
    "\n",
    "        q_text = self.qmap[qid]\n",
    "        d_text = self.dmap[pid]\n",
    "\n",
    "        x = encode_pair_cross(q_text, d_text, max_len=self.max_len)\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "print(\"Dataset class ready âœ…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42f0cb99-24e6-4817-94b6-c3849310c990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEncoder created âœ… | vocab_size: 18146\n"
     ]
    }
   ],
   "source": [
    "class CrossEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hidden=128, pad_id=0):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 4)  # classes 0-3\n",
    "        )\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        x = self.emb(token_ids)  # (B, L, D)\n",
    "        mask = (token_ids != self.pad_id).float().unsqueeze(-1)\n",
    "        pooled = (x * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1.0)\n",
    "        return self.mlp(pooled)\n",
    "\n",
    "model = CrossEncoder(vocab_size=len(cross_vocab), emb_dim=128, hidden=128, pad_id=PAD_ID)\n",
    "print(\"CrossEncoder created âœ… | vocab_size:\", len(cross_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0936edfd-463f-40de-bbbe-cf80deec688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X: torch.Size([256, 96]) Batch y: torch.Size([256])\n",
      "Token min/max: 0 18145  | vocab_size: 18146\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    CrossEncoderDataset(train_df, query_text_map, deal_text_map, max_len=96),\n",
    "    batch_size=256, shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    CrossEncoderDataset(val_df, query_text_map, deal_text_map, max_len=96),\n",
    "    batch_size=256, shuffle=False\n",
    ")\n",
    "\n",
    "Xb, yb = next(iter(train_loader))\n",
    "print(\"Batch X:\", Xb.shape, \"Batch y:\", yb.shape)\n",
    "print(\"Token min/max:\", Xb.min().item(), Xb.max().item(), \" | vocab_size:\", len(cross_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a4e4d48-baa3-4042-870e-203cfa6ab8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss=1.2302 acc=0.5002 | val loss=1.2198 acc=0.5001\n",
      "  âœ… saved cross_encoder_best.pt\n",
      "Epoch 2: train loss=1.1975 acc=0.5033 | val loss=1.1860 acc=0.5050\n",
      "  âœ… saved cross_encoder_best.pt\n",
      "Epoch 3: train loss=1.1632 acc=0.5115 | val loss=1.1780 acc=0.5074\n",
      "  âœ… saved cross_encoder_best.pt\n",
      "Epoch 4: train loss=1.1449 acc=0.5171 | val loss=1.1698 acc=0.5097\n",
      "  âœ… saved cross_encoder_best.pt\n",
      "Epoch 5: train loss=1.1329 acc=0.5216 | val loss=1.1673 acc=0.5113\n",
      "  âœ… saved cross_encoder_best.pt\n",
      "Epoch 6: train loss=1.1235 acc=0.5254 | val loss=1.1674 acc=0.5125\n",
      "Epoch 7: train loss=1.1150 acc=0.5287 | val loss=1.1674 acc=0.5124\n",
      "  ðŸ›‘ early stopping\n",
      "Training finished âœ…\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def run_epoch(model, loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X, y in loader:\n",
    "        if train:\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                logits = model(X)\n",
    "                loss = criterion(logits, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "os.makedirs(CKPT_PATH, exist_ok=True)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 2\n",
    "bad_epochs = 0\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    tr_loss, tr_acc = run_epoch(model, train_loader, train=True)\n",
    "    va_loss, va_acc = run_epoch(model, val_loader, train=False)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: train loss={tr_loss:.4f} acc={tr_acc:.4f} | val loss={va_loss:.4f} acc={va_acc:.4f}\")\n",
    "\n",
    "    if va_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = va_loss\n",
    "        bad_epochs = 0\n",
    "        torch.save(model.state_dict(), f\"{CKPT_PATH}/cross_encoder_best.pt\")\n",
    "        print(\"  âœ… saved cross_encoder_best.pt\")\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "        if bad_epochs >= patience:\n",
    "            print(\"  ðŸ›‘ early stopping\")\n",
    "            break\n",
    "\n",
    "print(\"Training finished âœ…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f3e504c-ecab-46a2-8f0b-b27e087d948a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cross_vocab_v1.json âœ…\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{CKPT_PATH}/cross_vocab_v1.json\", \"w\") as f:\n",
    "    json.dump(cross_vocab, f)\n",
    "\n",
    "print(\"Saved cross_vocab_v1.json âœ…\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
