{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fcfd893-adee-40d2-a884-0a30a5c7b6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2365f67e2d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, re, json, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "np.random.seed(7)\n",
    "torch.manual_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78a6c59-b330-425a-a9e1-9ca4f44d2619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 16) (15000, 27) (480000, 4)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../data/nexkey_synthetic_dataset_v1\"\n",
    "CKPT_PATH = \"../models/checkpoints\"\n",
    "\n",
    "queries = pd.read_csv(f\"{DATA_PATH}/queries.csv\")\n",
    "properties = pd.read_csv(f\"{DATA_PATH}/properties.csv\")\n",
    "interactions = pd.read_csv(f\"{DATA_PATH}/interactions.csv\")\n",
    "\n",
    "print(queries.shape, properties.shape, interactions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68d3e2c7-063b-46e3-9378-c6f184ba75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def property_to_text(row):\n",
    "    return (\n",
    "        f\"{row['deal_type']} {row['property_type']} in {row['city']} {row['state']}. \"\n",
    "        f\"{int(row['beds'])} bed {row['baths']} bath, {int(row['sqft'])} sqft. \"\n",
    "        f\"Purchase {int(row['purchase_price'])}, ARV {int(row['arv'])}, \"\n",
    "        f\"Entry {int(row['entry_fee'])}, Payment {row['estimated_monthly_payment']}. \"\n",
    "        f\"Condition {row['condition']}, Occupancy {row['occupancy']}.\"\n",
    "    )\n",
    "\n",
    "properties[\"deal_text\"] = properties.apply(property_to_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53657506-cf51-4e72-9837-6deb44b7eabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_int: (48000, 4)\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(7)\n",
    "all_qids = queries[\"query_id\"].unique()\n",
    "rng.shuffle(all_qids)\n",
    "\n",
    "n = len(all_qids)\n",
    "train_qids = set(all_qids[:int(0.80*n)])\n",
    "val_qids   = set(all_qids[int(0.80*n):int(0.90*n)])\n",
    "test_qids  = set(all_qids[int(0.90*n):])\n",
    "\n",
    "test_int = interactions[interactions[\"query_id\"].isin(test_qids)].copy()\n",
    "print(\"test_int:\", test_int.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4319005f-a43d-4b52-bceb-a8a39d1ffbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test queries with >=1 relevant: 3000\n"
     ]
    }
   ],
   "source": [
    "gt_test = (\n",
    "    test_int[test_int[\"relevance\"] >= 2]\n",
    "    .groupby(\"query_id\")[\"property_id\"]\n",
    "    .apply(set)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "test_query_ids = list(gt_test.keys())\n",
    "print(\"Test queries with >=1 relevant:\", len(test_query_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad1e9e35-a295-4832-81b1-14afeea11de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(ranked_pids, relevant_set, k):\n",
    "    return 1.0 if len(set(ranked_pids[:k]) & relevant_set) > 0 else 0.0\n",
    "\n",
    "def dcg_at_k(ranked_pids, relevant_set, k):\n",
    "    dcg = 0.0\n",
    "    for i, pid in enumerate(ranked_pids[:k], start=1):\n",
    "        rel = 1.0 if pid in relevant_set else 0.0\n",
    "        dcg += rel / np.log2(i + 1)\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(ranked_pids, relevant_set, k):\n",
    "    dcg = dcg_at_k(ranked_pids, relevant_set, k)\n",
    "    ideal_hits = min(len(relevant_set), k)\n",
    "    idcg = sum(1.0 / np.log2(i + 1) for i in range(1, ideal_hits + 1))\n",
    "    return dcg / idcg if idcg > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1cdc81d-224c-47a2-b9bc-8353e0a85648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dual_vocab: 18145\n",
      "cross_vocab: 18146\n",
      "deal_vecs: (15000, 128)\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{CKPT_PATH}/dual_vocab_v1.json\", \"r\") as f:\n",
    "    dual_vocab = json.load(f)\n",
    "\n",
    "deal_vecs = np.load(f\"{CKPT_PATH}/deal_vecs_v1.npy\")\n",
    "deal_vecs = deal_vecs / (np.linalg.norm(deal_vecs, axis=1, keepdims=True) + 1e-9)\n",
    "\n",
    "# Cross vocab (saved from notebook 08)\n",
    "cross_vocab_path = f\"{CKPT_PATH}/cross_vocab_v1.json\"\n",
    "if os.path.exists(cross_vocab_path):\n",
    "    with open(cross_vocab_path, \"r\") as f:\n",
    "        cross_vocab = json.load(f)\n",
    "else:\n",
    "    cross_vocab = copy.deepcopy(dual_vocab)\n",
    "    if \"<SEP>\" not in cross_vocab:\n",
    "        cross_vocab[\"<SEP>\"] = len(cross_vocab)\n",
    "\n",
    "PAD_ID_DUAL = dual_vocab[\"<PAD>\"]\n",
    "UNK_ID_DUAL = dual_vocab[\"<UNK>\"]\n",
    "\n",
    "PAD_ID = cross_vocab[\"<PAD>\"]\n",
    "UNK_ID = cross_vocab[\"<UNK>\"]\n",
    "SEP_ID = cross_vocab[\"<SEP>\"]\n",
    "\n",
    "print(\"dual_vocab:\", len(dual_vocab))\n",
    "print(\"cross_vocab:\", len(cross_vocab))\n",
    "print(\"deal_vecs:\", deal_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89c5eb56-c93b-4c3c-a0a8-916984d035c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str):\n",
    "    return re.findall(r\"[a-z0-9]+\", str(text).lower())\n",
    "\n",
    "def encode_text_dual(text: str, max_len: int = 48):\n",
    "    ids = [dual_vocab.get(w, UNK_ID_DUAL) for w in tokenize(text)][:max_len]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [PAD_ID_DUAL] * (max_len - len(ids))\n",
    "    return np.array(ids, dtype=np.int64)\n",
    "\n",
    "def encode_pair_cross(query_text: str, deal_text: str, max_len: int = 96):\n",
    "    q_ids = [cross_vocab.get(w, UNK_ID) for w in tokenize(query_text)]\n",
    "    d_ids = [cross_vocab.get(w, UNK_ID) for w in tokenize(deal_text)]\n",
    "\n",
    "    q_max = int(max_len * 0.45)\n",
    "    d_max = max_len - q_max - 1\n",
    "\n",
    "    q_ids = q_ids[:q_max]\n",
    "    d_ids = d_ids[:d_max]\n",
    "\n",
    "    ids = q_ids + [SEP_ID] + d_ids\n",
    "    if len(ids) < max_len:\n",
    "        ids += [PAD_ID] * (max_len - len(ids))\n",
    "\n",
    "    ids = np.array(ids, dtype=np.int64)\n",
    "    ids = np.clip(ids, 0, len(cross_vocab) - 1)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "741d4126-ee6a-4733-9b3c-945070038d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dual + cross ✅\n"
     ]
    }
   ],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, pad_id=0):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        x = self.emb(token_ids)\n",
    "        mask = (token_ids != self.pad_id).float().unsqueeze(-1)\n",
    "        summed = (x * mask).sum(dim=1)\n",
    "        denom = mask.sum(dim=1).clamp(min=1.0)\n",
    "        return summed / denom\n",
    "\n",
    "class DualEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, pad_id=0):\n",
    "        super().__init__()\n",
    "        self.query_encoder = TextEncoder(vocab_size, emb_dim, pad_id=pad_id)\n",
    "        self.deal_encoder  = TextEncoder(vocab_size, emb_dim, pad_id=pad_id)\n",
    "\n",
    "dual = DualEncoder(vocab_size=len(dual_vocab), emb_dim=128, pad_id=PAD_ID_DUAL)\n",
    "dual.load_state_dict(torch.load(f\"{CKPT_PATH}/dual_encoder_v1.pt\", map_location=\"cpu\"))\n",
    "dual.eval()\n",
    "\n",
    "class CrossEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hidden=128, pad_id=0):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        x = self.emb(token_ids)\n",
    "        mask = (token_ids != self.pad_id).float().unsqueeze(-1)\n",
    "        pooled = (x * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1.0)\n",
    "        return self.mlp(pooled)\n",
    "\n",
    "cross = CrossEncoder(vocab_size=len(cross_vocab), emb_dim=128, hidden=128, pad_id=PAD_ID)\n",
    "cross.load_state_dict(torch.load(f\"{CKPT_PATH}/cross_encoder_best.pt\", map_location=\"cpu\"))\n",
    "cross.eval()\n",
    "\n",
    "print(\"Loaded dual + cross ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "701a61ca-4c28-4ebd-a1da-4dda2c7023ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN_DUAL = 48\n",
    "MAX_LEN_CROSS = 96\n",
    "\n",
    "def retrieve_top_n(prompt: str, top_n: int = 50):\n",
    "    q_ids = torch.tensor(encode_text_dual(prompt, MAX_LEN_DUAL), dtype=torch.long).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_vec = dual.query_encoder(q_ids).cpu().numpy()\n",
    "    q_vec = q_vec / (np.linalg.norm(q_vec, axis=1, keepdims=True) + 1e-9)\n",
    "\n",
    "    sims = (deal_vecs @ q_vec.T).squeeze(1)\n",
    "    idx = np.argsort(-sims)[:top_n]\n",
    "    return idx\n",
    "\n",
    "def rerank_top_k(prompt: str, top_n: int = 50, top_k: int = 5):\n",
    "    idx = retrieve_top_n(prompt, top_n=top_n)\n",
    "\n",
    "    batch = []\n",
    "    for i in idx:\n",
    "        batch.append(encode_pair_cross(prompt, properties.iloc[i][\"deal_text\"], MAX_LEN_CROSS))\n",
    "\n",
    "    X = torch.tensor(np.stack(batch), dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        logits = cross(X)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "    expected_rel = (probs * np.array([0,1,2,3], dtype=np.float32)).sum(axis=1)\n",
    "    order = np.argsort(-expected_rel)[:top_k]\n",
    "    final_idx = idx[order]\n",
    "    return properties.iloc[final_idx][\"property_id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "226fc9d7-136f-473f-9de8-96fb67380d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ✅\n"
     ]
    }
   ],
   "source": [
    "Ks = [1, 5, 10]\n",
    "NDCG_K = 5\n",
    "\n",
    "# sample for speed; increase later\n",
    "EVAL_N = 1000 if len(test_query_ids) > 1000 else len(test_query_ids)\n",
    "eval_qids = np.random.choice(test_query_ids, size=EVAL_N, replace=False)\n",
    "\n",
    "rec_dual = {k: [] for k in Ks}\n",
    "rec_rerank = {k: [] for k in Ks}\n",
    "ndcg_dual = []\n",
    "ndcg_rerank = []\n",
    "\n",
    "for qid in eval_qids:\n",
    "    prompt = queries.loc[queries[\"query_id\"] == qid, \"query_text\"].iloc[0]\n",
    "    relevant = gt_test[qid]\n",
    "\n",
    "    # dual-only ranking = top 10 retrieved property IDs\n",
    "    idx_dual = retrieve_top_n(prompt, top_n=10)\n",
    "    ranked_dual_pids = properties.iloc[idx_dual][\"property_id\"].tolist()\n",
    "\n",
    "    # rerank = top 5\n",
    "    ranked_rerank_pids = rerank_top_k(prompt, top_n=50, top_k=10)  # get 10 to compute recall@10\n",
    "\n",
    "    for k in Ks:\n",
    "        rec_dual[k].append(recall_at_k(ranked_dual_pids, relevant, k))\n",
    "        rec_rerank[k].append(recall_at_k(ranked_rerank_pids, relevant, k))\n",
    "\n",
    "    ndcg_dual.append(ndcg_at_k(ranked_dual_pids, relevant, NDCG_K))\n",
    "    ndcg_rerank.append(ndcg_at_k(ranked_rerank_pids, relevant, NDCG_K))\n",
    "\n",
    "print(\"Done ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59445910-31f8-4136-8e93-892e7cc109b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall@1</th>\n",
       "      <th>Recall@5</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>NDCG@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dual only (retrieval)</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.01292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dual + Cross (rerank)</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.01753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Recall@1  Recall@5  Recall@10   NDCG@5\n",
       "0  Dual only (retrieval)     0.012     0.063      0.132  0.01292\n",
       "1  Dual + Cross (rerank)     0.022     0.077      0.148  0.01753"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean(x): \n",
    "    return float(np.mean(x)) if len(x) else 0.0\n",
    "\n",
    "rows = []\n",
    "rows.append({\n",
    "    \"Model\": \"Dual only (retrieval)\",\n",
    "    \"Recall@1\": mean(rec_dual[1]),\n",
    "    \"Recall@5\": mean(rec_dual[5]),\n",
    "    \"Recall@10\": mean(rec_dual[10]),\n",
    "    \"NDCG@5\": mean(ndcg_dual),\n",
    "})\n",
    "rows.append({\n",
    "    \"Model\": \"Dual + Cross (rerank)\",\n",
    "    \"Recall@1\": mean(rec_rerank[1]),\n",
    "    \"Recall@5\": mean(rec_rerank[5]),\n",
    "    \"Recall@10\": mean(rec_rerank[10]),\n",
    "    \"NDCG@5\": mean(ndcg_rerank),\n",
    "})\n",
    "\n",
    "report = pd.DataFrame(rows)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
