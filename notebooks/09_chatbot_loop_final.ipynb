{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0c6a306-dce1-40cb-92cf-1b80eb2eae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f842dc4b-1e37-49ac-b72f-58a5577d52d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ✅\n",
      "properties: (15000, 28)\n",
      "deal_vecs : (15000, 128)\n",
      "dual_vocab: 18145\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../data/nexkey_synthetic_dataset_v1\"\n",
    "CKPT_PATH = \"../models/checkpoints\"\n",
    "\n",
    "queries = pd.read_csv(f\"{DATA_PATH}/queries.csv\")\n",
    "properties = pd.read_csv(f\"{DATA_PATH}/properties.csv\")\n",
    "\n",
    "# Build deal_text (must match training)\n",
    "def property_to_text(row):\n",
    "    return (\n",
    "        f\"{row['deal_type']} {row['property_type']} in {row['city']} {row['state']}. \"\n",
    "        f\"{int(row['beds'])} bed {row['baths']} bath, {int(row['sqft'])} sqft. \"\n",
    "        f\"Purchase {int(row['purchase_price'])}, ARV {int(row['arv'])}, \"\n",
    "        f\"Entry {int(row['entry_fee'])}, Payment {row['estimated_monthly_payment']}. \"\n",
    "        f\"Condition {row['condition']}, Occupancy {row['occupancy']}.\"\n",
    "    )\n",
    "\n",
    "properties[\"deal_text\"] = properties.apply(property_to_text, axis=1)\n",
    "\n",
    "# Load dual vocab + deal vecs\n",
    "with open(f\"{CKPT_PATH}/dual_vocab_v1.json\", \"r\") as f:\n",
    "    dual_vocab = json.load(f)\n",
    "\n",
    "deal_vecs = np.load(f\"{CKPT_PATH}/deal_vecs_v1.npy\")\n",
    "deal_vecs = deal_vecs / (np.linalg.norm(deal_vecs, axis=1, keepdims=True) + 1e-9)\n",
    "\n",
    "print(\"Loaded ✅\")\n",
    "print(\"properties:\", properties.shape)\n",
    "print(\"deal_vecs :\", deal_vecs.shape)\n",
    "print(\"dual_vocab:\", len(dual_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d86f838-fe43-47c2-a5fa-e16b97255998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_vocab: 18146 | PAD/UNK/SEP: 0 1 18145\n"
     ]
    }
   ],
   "source": [
    "# If you saved cross_vocab_v1.json in notebook 08, load it:\n",
    "cross_vocab_path = f\"{CKPT_PATH}/cross_vocab_v1.json\"\n",
    "\n",
    "if os.path.exists(cross_vocab_path):\n",
    "    with open(cross_vocab_path, \"r\") as f:\n",
    "        cross_vocab = json.load(f)\n",
    "else:\n",
    "    # fallback: derive from dual vocab\n",
    "    cross_vocab = copy.deepcopy(dual_vocab)\n",
    "    if \"<SEP>\" not in cross_vocab:\n",
    "        cross_vocab[\"<SEP>\"] = len(cross_vocab)\n",
    "\n",
    "PAD_ID_DUAL = dual_vocab[\"<PAD>\"]\n",
    "UNK_ID_DUAL = dual_vocab[\"<UNK>\"]\n",
    "\n",
    "PAD_ID = cross_vocab[\"<PAD>\"]\n",
    "UNK_ID = cross_vocab[\"<UNK>\"]\n",
    "SEP_ID = cross_vocab[\"<SEP>\"]\n",
    "\n",
    "print(\"cross_vocab:\", len(cross_vocab), \"| PAD/UNK/SEP:\", PAD_ID, UNK_ID, SEP_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "073f2e34-64f3-4c2d-9255-1d90acbe1b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str):\n",
    "    return re.findall(r\"[a-z0-9]+\", str(text).lower())\n",
    "\n",
    "# Dual encoder query encoding (must match dual training exactly)\n",
    "def encode_text_dual(text: str, max_len: int = 48):\n",
    "    ids = [dual_vocab.get(w, UNK_ID_DUAL) for w in tokenize(text)][:max_len]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [PAD_ID_DUAL] * (max_len - len(ids))\n",
    "    return np.array(ids, dtype=np.int64)\n",
    "\n",
    "# Cross encoder pair encoding (must match cross training)\n",
    "def encode_pair_cross(query_text: str, deal_text: str, max_len: int = 96):\n",
    "    q_ids = [cross_vocab.get(w, UNK_ID) for w in tokenize(query_text)]\n",
    "    d_ids = [cross_vocab.get(w, UNK_ID) for w in tokenize(deal_text)]\n",
    "\n",
    "    q_max = int(max_len * 0.45)\n",
    "    d_max = max_len - q_max - 1\n",
    "\n",
    "    q_ids = q_ids[:q_max]\n",
    "    d_ids = d_ids[:d_max]\n",
    "\n",
    "    ids = q_ids + [SEP_ID] + d_ids\n",
    "    if len(ids) < max_len:\n",
    "        ids += [PAD_ID] * (max_len - len(ids))\n",
    "\n",
    "    ids = np.array(ids, dtype=np.int64)\n",
    "    ids = np.clip(ids, 0, len(cross_vocab) - 1)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44b44696-a7cf-4343-b22a-07957bcf1ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dual encoder loaded ✅\n"
     ]
    }
   ],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, pad_id=0):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        x = self.emb(token_ids)\n",
    "        mask = (token_ids != self.pad_id).float().unsqueeze(-1)\n",
    "        summed = (x * mask).sum(dim=1)\n",
    "        denom = mask.sum(dim=1).clamp(min=1.0)\n",
    "        return summed / denom\n",
    "\n",
    "class DualEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, pad_id=0):\n",
    "        super().__init__()\n",
    "        self.query_encoder = TextEncoder(vocab_size, emb_dim, pad_id=pad_id)\n",
    "        self.deal_encoder  = TextEncoder(vocab_size, emb_dim, pad_id=pad_id)\n",
    "\n",
    "dual = DualEncoder(vocab_size=len(dual_vocab), emb_dim=128, pad_id=PAD_ID_DUAL)\n",
    "dual.load_state_dict(torch.load(f\"{CKPT_PATH}/dual_encoder_v1.pt\", map_location=\"cpu\"))\n",
    "dual.eval()\n",
    "\n",
    "print(\"Dual encoder loaded ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03e5c3f9-f931-4c02-9732-2baa31a44c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross encoder loaded ✅\n"
     ]
    }
   ],
   "source": [
    "class CrossEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hidden=128, pad_id=0):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        x = self.emb(token_ids)\n",
    "        mask = (token_ids != self.pad_id).float().unsqueeze(-1)\n",
    "        pooled = (x * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1.0)\n",
    "        return self.mlp(pooled)\n",
    "\n",
    "cross = CrossEncoder(vocab_size=len(cross_vocab), emb_dim=128, hidden=128, pad_id=PAD_ID)\n",
    "cross.load_state_dict(torch.load(f\"{CKPT_PATH}/cross_encoder_best.pt\", map_location=\"cpu\"))\n",
    "cross.eval()\n",
    "\n",
    "print(\"Cross encoder loaded ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03edd4b4-515d-4887-b650-6c7239ab5bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN_DUAL = 48\n",
    "MAX_LEN_CROSS = 96\n",
    "\n",
    "def retrieve_top_n(prompt: str, top_n: int = 50):\n",
    "    q_ids = torch.tensor(encode_text_dual(prompt, max_len=MAX_LEN_DUAL), dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        q_vec = dual.query_encoder(q_ids).cpu().numpy()\n",
    "\n",
    "    q_vec = q_vec / (np.linalg.norm(q_vec, axis=1, keepdims=True) + 1e-9)\n",
    "    sims = (deal_vecs @ q_vec.T).squeeze(1)\n",
    "    idx = np.argsort(-sims)[:top_n]\n",
    "    return idx, sims[idx]\n",
    "\n",
    "def rerank_with_cross(prompt: str, top_n: int = 50, top_k: int = 5):\n",
    "    idx, sims = retrieve_top_n(prompt, top_n=top_n)\n",
    "\n",
    "    # Build cross inputs\n",
    "    batch = []\n",
    "    for i in idx:\n",
    "        d_text = properties.iloc[i][\"deal_text\"]\n",
    "        batch.append(encode_pair_cross(prompt, d_text, max_len=MAX_LEN_CROSS))\n",
    "\n",
    "    X = torch.tensor(np.stack(batch), dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = cross(X)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "    expected_rel = (probs * np.array([0,1,2,3], dtype=np.float32)).sum(axis=1)\n",
    "\n",
    "    order = np.argsort(-expected_rel)[:top_k]\n",
    "    final_idx = idx[order]\n",
    "\n",
    "    out = properties.iloc[final_idx].copy()\n",
    "    out[\"retrieval_sim\"] = sims[order]\n",
    "    out[\"rerank_score\"] = expected_rel[order]\n",
    "\n",
    "    cols = [\n",
    "        \"property_id\",\"deal_type\",\"city\",\"state\",\"beds\",\"baths\",\"sqft\",\n",
    "        \"purchase_price\",\"arv\",\"entry_fee\",\"estimated_monthly_payment\",\n",
    "        \"retrieval_sim\",\"rerank_score\"\n",
    "    ]\n",
    "    return out[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c58d3b72-eefb-4a80-9bbe-59577c4a3333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_missing_criteria(text: str):\n",
    "    t = text.lower()\n",
    "\n",
    "    has_beds = re.search(r\"\\d+\\s*\\+?\\s*bed\", t) is not None\n",
    "    has_price = re.search(r\"(under|max|<=)\\s*\\$?\\s*[\\d\\.,]+[km]?\", t) is not None or \"$\" in t\n",
    "    has_location = any(k in t for k in [\n",
    "        \"az\",\"arizona\",\"tx\",\"texas\",\"fl\",\"florida\",\"ga\",\"georgia\",\"nc\",\"north carolina\",\n",
    "        \"sc\",\"south carolina\",\"tn\",\"tennessee\",\"ca\",\"california\"\n",
    "    ])\n",
    "\n",
    "    missing = []\n",
    "    if not has_location: missing.append(\"location (city/state)\")\n",
    "    if not has_beds: missing.append(\"bedrooms (e.g., 3 bed)\")\n",
    "    if not has_price: missing.append(\"max purchase price (e.g., under 350k)\")\n",
    "    return missing\n",
    "\n",
    "def format_deals(df):\n",
    "    lines = []\n",
    "    for i, row in df.reset_index(drop=True).iterrows():\n",
    "        lines.append(\n",
    "            f\"{i+1}) {row['deal_type']} | {row['beds']}bd/{row['baths']}ba | {row['city']}, {row['state']} | \"\n",
    "            f\"Buy ${int(row['purchase_price']):,} | ARV ${int(row['arv']):,} | \"\n",
    "            f\"Entry ${int(row['entry_fee']):,} | Pay ${float(row['estimated_monthly_payment']):,.0f} \"\n",
    "            f\"| Score {row['rerank_score']:.2f}\"\n",
    "        )\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def chat_reply(user_message: str):\n",
    "    missing = detect_missing_criteria(user_message)\n",
    "\n",
    "    # If too vague: ask clarifying question\n",
    "    if len(missing) >= 2:\n",
    "        return (\n",
    "            \"To find the best deals, I need a bit more detail.\\n\"\n",
    "            f\"Can you share: {', '.join(missing)}?\\n\"\n",
    "            \"Example: “3 bed in AZ under 350k, entry under 20k, payment under 2500”\"\n",
    "        )\n",
    "\n",
    "    # Otherwise attempt retrieval + rerank\n",
    "    deals = rerank_with_cross(user_message, top_n=50, top_k=5)\n",
    "\n",
    "    if deals is None or len(deals) == 0:\n",
    "        return (\n",
    "            \"I couldn’t find any deals that match those criteria.\\n\"\n",
    "            \"Try loosening one constraint:\\n\"\n",
    "            \"- increase max price (e.g., +50k)\\n\"\n",
    "            \"- increase entry limit (e.g., +10k)\\n\"\n",
    "            \"- allow higher payment (e.g., +300/month)\\n\"\n",
    "            \"- broaden location (state instead of city)\"\n",
    "        )\n",
    "\n",
    "    return \"Here are the top 5 deals I found:\\n\\n\" + format_deals(deals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667061fa-68ed-45e1-8609-edbfe85ca69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NexKey DealMatch Chatbot (type 'quit' to exit)\n"
     ]
    }
   ],
   "source": [
    "print(\"NexKey DealMatch Chatbot (type 'quit' to exit)\")\n",
    "\n",
    "while True:\n",
    "    msg = input(\"\\nYou: \").strip()\n",
    "    if msg.lower() in [\"quit\", \"exit\"]:\n",
    "        print(\"Bot: Bye!\")\n",
    "        break\n",
    "\n",
    "    print(\"\\nBot:\", chat_reply(msg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
